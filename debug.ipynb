{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd51a95-4379-4ff4-97a5-c27061571168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.workflow import *\n",
    "from src.datasets.tshark import TsharkData\n",
    "from src.models.classification import RandomForest\n",
    "\n",
    "dataset = TsharkData(\"Data/tshark/scenario1\", 0.3, \"multi\")\n",
    "model = RandomForest()\n",
    "\n",
    "wf = WorkFlow(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf2597f-d927-4f3a-adef-24f8e5d9df4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aaron/projecten/ml-ot'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b38c489-0e47-43d2-8473-a57112ac2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"/home/aaron/projecten/ml-ot/multirun/2023-04-06/10-23-32/0/../TsharkData_supervised.pkl\"\n",
    "output_dir = \"/home/aaron/projecten/ml-ot/multirun/2023-04-06/10-23-32/0\"\n",
    "import pickle\n",
    "\n",
    "if \"multirun\" in output_dir and os.path.exists(path_data):\n",
    "    with open(path_data, 'rb') as f:\n",
    "        dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc6ed72-320e-4bd8-8ba3-94a37874c4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame.len</th>\n",
       "      <th>label</th>\n",
       "      <th>packets_per_min</th>\n",
       "      <th>packets_per_sec</th>\n",
       "      <th>max_packets</th>\n",
       "      <th>min_packets</th>\n",
       "      <th>mean_packets</th>\n",
       "      <th>packets_per_ip.dst</th>\n",
       "      <th>SYN</th>\n",
       "      <th>ACK</th>\n",
       "      <th>FIN</th>\n",
       "      <th>PSH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>822.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>824.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.846154</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>824.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.846154</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.857143</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5283</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.857143</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5284 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      frame.len  label  packets_per_min  packets_per_sec  max_packets  \\\n",
       "0            74      0              2.0              0.0          0.0   \n",
       "1            74      0              2.0              0.0          0.0   \n",
       "2            74      0              4.0              2.0          2.0   \n",
       "3            74      0              4.0              2.0          2.0   \n",
       "4            66      0              6.0              4.0          4.0   \n",
       "...         ...    ...              ...              ...          ...   \n",
       "5279         66      0            822.0             22.0         22.0   \n",
       "5280         66      0            824.0             24.0         24.0   \n",
       "5281         66      0            824.0             24.0         24.0   \n",
       "5282         66      0            826.0             26.0         26.0   \n",
       "5283         66      0            826.0             26.0         26.0   \n",
       "\n",
       "      min_packets  mean_packets  packets_per_ip.dst  SYN  ACK  FIN  PSH  \n",
       "0             0.0      0.000000                 0.0  1.0  0.0  0.0  0.0  \n",
       "1             0.0      0.000000                 0.0  1.0  0.0  0.0  0.0  \n",
       "2             0.0      1.000000                 0.0  1.0  1.0  0.0  0.0  \n",
       "3             0.0      1.000000                 0.0  1.0  1.0  0.0  0.0  \n",
       "4             0.0      2.000000                 2.0  0.0  1.0  0.0  0.0  \n",
       "...           ...           ...                 ...  ...  ...  ...  ...  \n",
       "5279          0.0     10.833333                 8.0  0.0  1.0  1.0  0.0  \n",
       "5280          0.0     11.846154                 6.0  0.0  1.0  1.0  0.0  \n",
       "5281          0.0     11.846154                 6.0  0.0  1.0  1.0  0.0  \n",
       "5282          0.0     12.857143                10.0  0.0  1.0  0.0  0.0  \n",
       "5283          0.0     12.857143                10.0  0.0  1.0  0.0  0.0  \n",
       "\n",
       "[5284 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1b5397-67b9-496a-a4c1-6f5f63acb5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmasker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlink\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCPUDispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0midentity\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7fe5a1480310\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlinearize_link\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Uses Shapley values to explain any machine learning model or python function.\n",
       "\n",
       "This is the primary explainer interface for the SHAP library. It takes any combination\n",
       "of a model and masker and returns a callable subclass object that implements\n",
       "the particular estimation algorithm that was chosen.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Build a new explainer for the passed model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "model : object or function\n",
       "    User supplied function or model object that takes a dataset of samples and\n",
       "    computes the output of the model for those samples.\n",
       "\n",
       "masker : function, numpy.array, pandas.DataFrame, tokenizer, None, or a list of these for each model input\n",
       "    The function used to \"mask\" out hidden features of the form `masked_args = masker(*model_args, mask=mask)`.\n",
       "    It takes input in the same form as the model, but for just a single sample with a binary\n",
       "    mask, then returns an iterable of masked samples. These\n",
       "    masked samples will then be evaluated using the model function and the outputs averaged.\n",
       "    As a shortcut for the standard masking using by SHAP you can pass a background data matrix\n",
       "    instead of a function and that matrix will be used for masking. Domain specific masking\n",
       "    functions are available in shap such as shap.ImageMasker for images and shap.TokenMasker\n",
       "    for text. In addition to determining how to replace hidden features, the masker can also\n",
       "    constrain the rules of the cooperative game used to explain the model. For example\n",
       "    shap.TabularMasker(data, hclustering=\"correlation\") will enforce a hierarchial clustering\n",
       "    of coalitions for the game (in this special case the attributions are known as the Owen values).\n",
       "\n",
       "link : function\n",
       "    The link function used to map between the output units of the model and the SHAP value units. By\n",
       "    default it is shap.links.identity, but shap.links.logit can be useful so that expectations are\n",
       "    computed in probability units while explanations remain in the (more naturally additive) log-odds\n",
       "    units. For more details on how link functions work see any overview of link functions for generalized\n",
       "    linear models.\n",
       "\n",
       "algorithm : \"auto\", \"permutation\", \"partition\", \"tree\", or \"linear\"\n",
       "    The algorithm used to estimate the Shapley values. There are many different algorithms that\n",
       "    can be used to estimate the Shapley values (and the related value for constrained games), each\n",
       "    of these algorithms have various tradeoffs and are preferrable in different situations. By\n",
       "    default the \"auto\" options attempts to make the best choice given the passed model and masker,\n",
       "    but this choice can always be overriden by passing the name of a specific algorithm. The type of\n",
       "    algorithm used will determine what type of subclass object is returned by this constructor, and\n",
       "    you can also build those subclasses directly if you prefer or need more fine grained control over\n",
       "    their options.\n",
       "\n",
       "output_names : None or list of strings\n",
       "    The names of the model outputs. For example if the model is an image classifier, then output_names would\n",
       "    be the names of all the output classes. This parameter is optional. When output_names is None then\n",
       "    the Explanation objects produced by this explainer will not have any output_names, which could effect\n",
       "    downstream plots.\n",
       "\n",
       "seed: None or int\n",
       "    seed for reproducibility\n",
       "\u001b[0;31mFile:\u001b[0m           ~/projecten/ml-ot/env_mlot/lib/python3.9/site-packages/shap/explainers/_explainer.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Permutation, Partition, Tree, Exact, Additive, Linear, Kernel, PyTorchDeep, TFDeep, Deep, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shap\n",
    "shap.Explainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503bac0a-465c-4acc-82c3-96821f4b53e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'class' column not present in dataset resorting to binary labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at log warning\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "HydraConfig was not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projecten/ml-ot/src/workflow.py:30\u001b[0m, in \u001b[0;36mWorkflow.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     29\u001b[0m predicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projecten/ml-ot/src/models/classification.py:27\u001b[0m, in \u001b[0;36mRandomForest.evaluate\u001b[0;34m(self, dataset, preds)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: InterfaceData, preds):\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mcf_report\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projecten/ml-ot/src/evaluation.py:13\u001b[0m, in \u001b[0;36mcf_report\u001b[0;34m(model, dataset, preds, class_values, target_values)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcf_report\u001b[39m(model: InterfaceModel, dataset: Dataset, preds: Union[\u001b[38;5;28mtuple\u001b[39m[array], array],\n\u001b[1;32m     11\u001b[0m              class_values: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target_values: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 13\u001b[0m     hydra_cfg \u001b[38;5;241m=\u001b[39m \u001b[43mhydra\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydra_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHydraConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m hydra_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruntime\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projecten/ml-ot/env_mlot/lib/python3.9/site-packages/hydra/core/hydra_config.py:31\u001b[0m, in \u001b[0;36mHydraConfig.get\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m instance \u001b[38;5;241m=\u001b[39m HydraConfig\u001b[38;5;241m.\u001b[39minstance()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHydraConfig was not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhydra\n",
      "\u001b[0;31mValueError\u001b[0m: HydraConfig was not set"
     ]
    }
   ],
   "source": [
    "wf.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a79419-5908-4a7b-9982-1085700df558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/nprobe/NF-UNSW-NB15.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74913a72-b7fa-4aa0-99c0-a5ecb42539e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "#preprocessing protocol layer 4\n",
    "ip_prot_map = {6:'TCP', 17:'UDP', 1:'ICMP', 2:'IGMP', 58:'IPv6-ICMP', 47:'GRE', 0:'HOPOPT'}\n",
    "df['PROTOCOL'] = df['PROTOCOL'].map(ip_prot_map).fillna('other')\n",
    "df = pd.get_dummies(df, columns=['PROTOCOL'])\n",
    "\n",
    "#dropping some columns\n",
    "df.drop(columns=['IPV4_SRC_ADDR','L4_SRC_PORT', 'IPV4_DST_ADDR', 'L4_DST_PORT'], inplace=True)\n",
    "\n",
    "#rename columns\n",
    "df.rename(columns={'Label':'label', 'Attack':'class'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8fb0e-f900-4e40-9c2a-f7cbbe1ee94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19909ec-2883-4b73-8d9e-fad4e18bd8b6",
   "metadata": {},
   "source": [
    "* We drop the flow identifiers such as IDs, source/destination IP and ports, timestamps and start/end time.\n",
    "* we utilise the min-max normalisation technique to scale all datasets’ values between 0 to 1\n",
    "* Due to the extreme imbalance in all datasets’ binary-class and multi-class labels, we set a custom class weight parameter, using Equation 1. To reliably evaluate the datasets.\n",
    " 1. Weight_class = #total samples/ (#classes * #class samples)\n",
    "* we conduct five cross-validation splits and collect the average metrics such as accuracy, Area Under the Curve (AUC), F1 Score, Detection Rate (DR), False Alarm Rate (FAR) and time required in microseconds (µs) to predict a single test sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d7471-a499-475f-90de-5eb30ac76bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, y = df.drop(columns=['label','class']), df['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a61f3-493b-49f0-b96a-44b6f09bae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "cols_scale = ['L7_PROTO','IN_BYTES','OUT_BYTES','IN_PKTS','OUT_PKTS','TCP_FLAGS',\n",
    "             'FLOW_DURATION_MILLISECONDS']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(x_train[cols_scale])\n",
    "x_train[cols_scale], x_test[cols_scale] = scaler.transform(x_train[cols_scale]), scaler.transform(x_test[cols_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e424a-c064-4d89-b578-0b9608f14dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01ab2d-23a7-4790-940b-a85a56281f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train, pred_test = model.predict(x_train), model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb597c44-bb38-4154-9085-ebc82e849029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cf_train = classification_report(y_train, pred_train)\n",
    "cf_test = classification_report(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4df1f-dd48-40c0-b27f-5b3227f3316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cf_train)\n",
    "print(cf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77dd7a5-c644-49af-8a8a-d477c4803169",
   "metadata": {},
   "source": [
    "# Explainable AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d3d29-397f-4a49-b929-79a26fd734d5",
   "metadata": {},
   "source": [
    "## Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b272a-93af-4695-af70-4b9062e4db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inladen data en getrained model\n",
    "import pickle\n",
    "with open(\"data.pkl\",\"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2849a-b800-4bdc-bc71-63cfa0ab8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "clf = model.model\n",
    "explainer = shap.Explainer(clf, dataset.x_train)\n",
    "values = explainer.shap_values(dataset.x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf9da1-4be7-4787-bf90-b29e0f3d98d1",
   "metadata": {},
   "source": [
    "### Normale traffiek\n",
    "Belangrijkste inputs voor het model om normale traffiek te voorspellen staan in het **blauw**.\\\n",
    "Inputs voor het model om abnormale traffiek te voorspellen staan in het **rood**.\n",
    "\n",
    "De belangrijkste inputs zijn hier\n",
    "\n",
    "- Frame.len: hoeveelheid data verstuurd\n",
    "- packets_per_min: aantal berichten in de afgelopen minuut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e0f21-4a68-405c-acb8-a444429b9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voorbeeld van normale trafiek\n",
    "display(shap.force_plot(explainer.expected_value[1], values[1][10,:], dataset.x_train.iloc[10,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d1da1-fff0-40df-8de9-9d955373e9be",
   "metadata": {},
   "source": [
    "### ARP scan\n",
    "Eerst wordt er een ARP scan uitgevoerd om te bepalen welke devices op het netwerk zitten en wat hun bijbehorende IP adres is.\\\n",
    "Zo weet de hacker welke devices mogelijks een vulnerability hebben.\n",
    "\n",
    "De inputs voor het model die leiden tot het voorspellen van een aanval zijn hier in het **rood** weergegeven.\n",
    "\n",
    "(te vragen aan tijl A. is dit correct, is een ARP scan af te lezen van TCP traffiek?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5c883-89c2-45bd-b4f1-bb6db6dc4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.y_train\n",
    "attack_rows = y.index[y.label==1].tolist()\n",
    "normal_rows = y.index[y.label==0].tolist()\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "#ARP scan\n",
    "idx = attack_rows[5]\n",
    "print(f\"## row number: {idx}\")\n",
    "display(shap.force_plot(explainer.expected_value[1], values[1][idx,:], dataset.x_train.iloc[idx,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae982c-2a1f-416c-82e4-b3cc96ef5e1c",
   "metadata": {},
   "source": [
    "### NMAP scan\n",
    "Vervolgens worden alle poorten van een specifiek modbus device uitgelezen. Andere input waardes zijn van belang hier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c4b72-ff5b-469f-818a-88d2bb68a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poorten modbus uitlezen\n",
    "idx = attack_rows[90]\n",
    "print(f\"## row number: {idx}\")\n",
    "display(shap.force_plot(explainer.expected_value[1], values[1][idx,:], dataset.x_train.iloc[idx,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ac91a-e8ba-4a84-88e9-46d88cd052ec",
   "metadata": {},
   "source": [
    "### Aanpassen registers\n",
    "Als laatste stap worden er een aantal registers overschreven door de aanvaller, opnieuw zien we dat andere input waardes van belang zijn hier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb0392-ca6d-49c2-a0eb-0afe291cf0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data registers overschrijven\n",
    "idx = attack_rows[400]\n",
    "print(f\"## row number: {idx}\")\n",
    "display(shap.force_plot(explainer.expected_value[1], values[1][idx,:], dataset.x_train.iloc[idx,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824246d6-dbf8-467c-8ef3-d1d80208788e",
   "metadata": {},
   "source": [
    "### Gemiddeld belang van input\n",
    "\n",
    "We kunnen kijken naar het gemiddeld belang van elke input over de hele dataset waar een ARP en NMAP scan plaatsvinden,\\\n",
    "alsook registers worden aangepast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004de302-85c9-48c0-abc4-bcfdfec1591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(values[1], dataset.x_train, class_names=model.get_class_names(), plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a20c9-7ec8-4532-ae4f-9c62462d5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schrijven van holding registers\n",
    "shap.force_plot(explainer.expected_value[1], values[1][3000:3500,:], dataset.x_train.iloc[3000:3500,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf200f5d-4f17-4211-9609-9a9c1200d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scan van registers\n",
    "shap.force_plot(explainer.expected_value[1], values[1][2000:3000,:], dataset.x_train.iloc[2000:3000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13c1ed-5cfe-4ce1-af55-3a6868286c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARP scan en begin scan\n",
    "shap.force_plot(explainer.expected_value[1], values[1][0:1100,:], dataset.x_train.iloc[0:1100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759cb04-d771-4fb9-b8cb-0b0de1160c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.label.plot()\n",
    "y = dataset.y_train\n",
    "attack_rows = y.index[y.label==1].tolist()\n",
    "normal_rows = y.index[y.label==0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38326b18-2eda-4875-8722-58c1e9ea08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train, pred_test = model.predict()\n",
    "x_train = dataset.x_train\n",
    "x_test = dataset.x_test\n",
    "mask = pred_train != dataset.y_train.label.values\n",
    "# mask = pred_test != dataset.y_test.label.values\n",
    "x_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42441e42-8aeb-443d-903f-d2a6ced82b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0e221-b87a-453a-80a9-8a0539a3371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_dir = 'multirun/2023-01-16/11-21-04/9/NprobeData.pkl'\n",
    "\n",
    "with open(data_dir,'rb') as f:\n",
    "    dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f797e19-8f45-446e-a62e-26c600b8a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee10bf1-faad-439c-ad65-59e128d66a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = dataset.x_train, dataset.x_test\n",
    "y_train, y_test = dataset.y_train, dataset.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9755b0-9a9a-471f-b534-143dc77a9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ecod = model.model\n",
    "train_scores, test_scores = ecod.decision_function(x_train), ecod.decision_function(x_test)\n",
    "\n",
    "print(ecod.threshold_)\n",
    "plt.hist(train_scores, bins='auto') # arguments are passed to np.histogram\n",
    "plt.title(\"Outlier score\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de998cd-1032-4c1d-a808-f6cbe7dbac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.model.threshold_ = 19.5\n",
    "pred_train, pred_test = model.predict()\n",
    "print(classification_report(y_train, pred_train))\n",
    "print(classification_report(y_test, pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5fd73d-5ad1-433f-aff2-d23b9fceb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stat_threshold(df, pred_score, threshold):\n",
    "    # Let's see how many '0's and '1's.\n",
    "    df = pd.DataFrame(df)\n",
    "    df['Anomaly_Score'] = pred_score\n",
    "    df['Group'] = np.where(df['Anomaly_Score'] < threshold, 'Normal', 'Outlier')\n",
    "\n",
    "    # Now let's show the summary statistics:\n",
    "    cnt = df.groupby('Group')['Anomaly_Score'].count().reset_index().rename(columns={'Anomaly_Score':'Count'})\n",
    "    cnt['Count %'] = (cnt['Count'] / cnt['Count'].sum()) * 100 # The count and count %\n",
    "    stat = df.groupby('Group').mean().round(2).reset_index() # The avg.\n",
    "    stat = cnt.merge(stat, left_on='Group',right_on='Group') # Put the count and the avg. together\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7948fa-5207-404a-8c26-dda69b71f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "descriptive_stat_threshold(x_train, train_scores, 19.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af43bf-379e-42dc-afb4-0e068a246652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1cb85-0f9e-492c-959a-9fbd7cadae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7a022-0299-4a49-8f95-9bae9c5e7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"multirun/2023-01-23/16-15-28/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b31b1b-fbc0-479a-8d6f-c4f6a374cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x, _  = next(os.walk('.'))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aaff1a-abf9-46dd-9408-1609b714950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(x[0]):\n",
    "    if file.endswith('.txt'):\n",
    "        lines = open(f'{x[0]}/{file}').readlines()\n",
    "        for line in lines:\n",
    "            if 'macro avg' in line:\n",
    "                print(line.split()[-2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5e780-472b-44d5-b808-fa58a46851d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "                \n",
    "                        \n",
    "\n",
    "plot_general_graph('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ecd7a-72ab-4358-894c-f8f77dd51763",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/aaron/projecten/ML-OT/multirun/2023-01-24/13-33-50')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d3e8f-a772-4f21-a69b-a52f54c3a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {'f1-macro':[0.97 ,0.93, 0.69, 0.71, 0.58, 0.58, 1.00, 0.98], 'set':[\"train\",\"test\"]*4, \n",
    "           'algo':[\"randomf\",\"randomf\",\"iforest\",\"iforest\", \"iforest\",\"iforest\", \"randomf\", \"randomf\"],\n",
    "          'dataset':[\"nprobe\",\"nprobe\",\"tshark\",\"tshark\", \"nprobe\",\"nprobe\",\"tshark\",\"tshark\"]}\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "import os\n",
    "\n",
    "def get_scores(path_run: str):\n",
    "    scores, algo, ds = [], [], []\n",
    "    \n",
    "    #get local directories 1-level deep\n",
    "    _, dirs, _ = next(os.walk(path_run))\n",
    "    for d in dirs:\n",
    "        for file in os.listdir(d):\n",
    "            if file.endswith('.txt'):\n",
    "                lines = open(f\"{d}/{file}\").readlines()\n",
    "                for line in lines:\n",
    "                    if 'macro avg' in line:\n",
    "                        scores.append(float(line.split()[-2]))\n",
    "                        algo.append(file[:-4].lower())\n",
    "                        ds.append(d.split('dataset=')[1].split(',')[0])\n",
    "                        \n",
    "    results = {'F1-macro':scores,\n",
    "              'Dataset':ds,\n",
    "              'Algorithm': algo,\n",
    "              'set':[\"train\",\"test\"]*int(len(scores)/2)}\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "def plot_overview(df: pd.DataFrame):\n",
    "    f, axs = plt.subplots(1, len(df.Dataset.unique()), sharey=True)\n",
    "    subplots = []\n",
    "    for idx , dataset in enumerate(df.Dataset.unique()):\n",
    "        subplots.append(sns.barplot(data=df[df.Dataset == dataset], x='Algorithm', y='F1-macro', hue='set', ax=axs[idx]))\n",
    "        subplots[idx].legend_.remove()\n",
    "        subplots[idx].set_title(dataset.upper())\n",
    "        for container in subplots[idx].containers:\n",
    "            subplots[idx].bar_label(container)\n",
    "            \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    f.tight_layout()\n",
    "    plt.savefig('overview-scores.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed7d46-3a97-4fcd-891d-452d496ae281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_scores('.')\n",
    "display(df)\n",
    "plot_overview(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d37b8f-1fc5-4e91-9835-fb1a5d12fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model.pkl','rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64d8fb-8683-405b-9bf1-e96b6be7cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#goeie grafiek als argument dat verkeer cyclisch is en afwijking hierop als anomaly kunnen gezien worden\n",
    "ds = data.data.reset_index()\n",
    "ds = ds[ds['ip.proto_tcp'] == 1]\n",
    "ds = ds.rename(columns={\"index\":\"packet no.\"})\n",
    "display(ds)\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.scatterplot(data= ds[ds['label']==1], x=\"packet no.\", y=\"mean_packets\", color=\"red\", marker=\"X\", label=\"aanval\")\n",
    "sns.lineplot(data = ds, x=\"packet no.\", y=\"mean_packets\", alpha=0.5, label=\"normale traffiek\")\n",
    "plt.title(\"Gemiddeld aantal paketten per minuut op netwerk\")\n",
    "plt.xlabel(\"Packet nummer\")\n",
    "plt.ylabel(\"Gemiddelde van paketten\")\n",
    "plt.savefig(\"cyclical.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f03e21-6ee9-4f9d-8df9-e3586c3b1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[ds['label']==1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a5e4f-5d98-43cd-b2d4-8c33894ae778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "\n",
    "IForest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15930d11-fe58-4f20-b1a5-b0142056350a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning in Operational Technology (ML-OT)\n",
    "ML-OT is a toolbox to apply different algorithms and analyses on network data.\n",
    "1. Different options for experiments can be set with the commandline and easily reproduced. The Open source project Hydra from facebook research provides this functionality.\n",
    "2. Datasets made by different parsers for network data are supported f.e. tshark, nProbe.\n",
    "3. Automatic evaluation and analyses o.a. statistics, explainable AI, ..\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install pytorch with instructions from official website https://pytorch.org/get-started/locally/\n",
    "This depends on specific operating system, hardware and software.\n",
    "```console\n",
    "pip install torch --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "```\n",
    "Install remaining libraries of the requirements file.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "This command turns on tab completion for hydra.\n",
    "```console\n",
    "eval \"$(python main.py -sc install=bash)\"\n",
    "```\n",
    "\n",
    "When you make use of an virtual environment you can add this command to the activation script.\n",
    "This will automatically activate tab completion when entering your virtual environment.\n",
    "Use the absolute pad to the `main.py` script.\n",
    "```console\n",
    "echo 'eval \"$(python <path_main_script> -sc install=bash)\"' | tee -a env/bin/activate\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Experiments with terminal\n",
    "You can smoothly select different options for experiments and tab completion is enabled. \n",
    "This recording is a short showcase of some simple experiments.\n",
    "[![asciicast](https://asciinema.org/a/Fa6K17UslsOcGqka3h8nmLak3.svg)](https://asciinema.org/a/Fa6K17UslsOcGqka3h8nmLak3)\n",
    "\n",
    "Using this procedure you can explore the different configurations.\n",
    "To apply one or more algorithms for one or more datasets add the `--multirun` arg to the command:\n",
    "```console\n",
    "python main.py --multirun dataset=tshark,nprobe model=randomforest,XGB\n",
    "```\n",
    "You can also do a **grid search** or sweep on a single algorithm:\n",
    "```console\n",
    "python main.py --multirun dataset=tshark model=randomforest model.n_estimators=2,5,10 model.max_depth=5,10,20\n",
    "```\n",
    "To perform a smarter hyperparameter search make use of `hp_search.py`:\n",
    "```console\n",
    "python hp_search.py --multirun dataset=tshark model=randomforest 'model.n_estimators=range(10,200)' 'model.max_depth=range(5,20)'\n",
    "```\n",
    "This will make use of the nevergrad plugin for hydra to search the parameter space and returns the parameter with the best average f1-score.\n",
    "You can find more information on how to use nevergrad [here](https://hydra.cc/docs/plugins/nevergrad_sweeper/#defining-the-parameters)\n",
    "\n",
    "### Experiments through config\n",
    "Hydra uses a collection of config files in the background for the tab completion in terminal.\n",
    "The config files can be adapted directly to have more extended control over the experiment.\n",
    "\n",
    "The `configs` directory contains three important subdirectories:\n",
    "1. `dataset`, contains the different options for datasets such as test size, data directory and whether labels should be binary or multi-class.\n",
    "2. `model`, contains the different hyperparameters.\n",
    "3. `analysis`, contains the different functions to apply after training for each model.\n",
    "\n",
    "> Note that there are two types of models: classification and anomaly. Anomaly can only handle binary labels and classification can handle both binary and multi-class labels. \n",
    "\n",
    "To adapt the parameter search space used by the hp_search.py script adapt the config file `configs/hydra/sweeper/hyperparam_search.yaml`.\n",
    "The default options here are for a randomforest model with the number of trees between 10 and 150. \n",
    "```yaml\n",
    "parametrization:\n",
    "    model : randomforest\n",
    "    model.n_estimators:\n",
    "        lower: 10\n",
    "        upper: 150\n",
    "        integer: True\n",
    "```\n",
    "\n",
    "You can also create a new config file, f.e. `my_search_space`, under the same directory and adapt the top level config file `configs/config_hpsearch.yaml` to:\n",
    "```yaml\n",
    "defaults:\n",
    "    - _self_\n",
    "    - dataset: tshark\n",
    "    - model: randomforest\n",
    "    - override hydra/sweeper: my_search_space  \n",
    "```\n",
    "\n",
    "This way you can maintain different experiments easily.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "In the `outputs` directory you can find results of experiments with a single option of configs.\n",
    "* main.log file containing all the executed steps.\n",
    "* An evaluation for the model, report on the scores for the different classes.\n",
    "* Analysis folder with different produced graphs.\n",
    "\n",
    "These graphs are produced by different functions and they can be changed from the config files in directory `configs/analysis` each of the models contain different config files.\n",
    "\n",
    "In the `multirun` directory you can find results of experiments with multiple options for config or sweeps. For grid search sweeps it will contain the same output as a normal experiment but in different subdirectories.For sweeps with nevergrad it will contain the different scores for each hyperparameter setting and the best found hyperparameters.\n",
    "\n",
    "\n",
    "## Extendability\n",
    "Datasets, models and analysis functions can be added to this project.\n",
    "\n",
    "### Dataset\n",
    "Dataset classes extend an interface.\n",
    "\n",
    "```python\n",
    "class InterfaceData(Dataset):\n",
    "    def __init__(self, data_path: str, test_size: float, label_type: str, shuffle: bool):\n",
    "        \"\"\"Initialize dataset object with properties\"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.test_size = test_size\n",
    "        self.label_type = label_type\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.data=None\n",
    "        self.x_train, self.x_test = None, None\n",
    "        self.y_train, self.y_test = None, None\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"method to retrieve length of data contained\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"method to retrieve row of data\"\"\"\n",
    "        return self.data.iloc[idx,:]           \n",
    "    \n",
    "    def load(self) -> None:\n",
    "        \"\"\"Load in dataset and store in object\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self) -> None:\n",
    "        \"\"\"Apply a specified preprocessing function and store in object\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def postprocess(self) -> None:\n",
    "        \"\"\"Any processing applied after splitting data, such as rescaling store in object\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "#### Step 1\n",
    "To add a new dataset, extend the InterfaceData class in a python file under `src/datasets/`\n",
    "\n",
    "#### Step 2\n",
    "Write seperate functions for loading, preprocessing and postprocessing. You could also adapt the constructor, most of the time this will not be necessary.\n",
    "\n",
    "#### Step 3\n",
    "Add a new yaml file in the directory `configs/dataset`.\\\n",
    "It should target the new dataset class you created, a data path, test size, label_type and shuffle argument.\n",
    "The tshark config looks like this:\n",
    "```yaml\n",
    "_target_: src.datasets.tshark.TsharkData\n",
    "data_path: 'Data/tshark/scenario1'\n",
    "label_type: 'binary'\n",
    "test_size: 0.3\n",
    "shuffle: False\n",
    "```\n",
    "\n",
    "### Model\n",
    "\n",
    "Model classes extend an interface. \n",
    "```python\n",
    "class InterfaceModel():\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\" Initialize model_type as 'supervised','semi-supervised' or 'unsupervised'\n",
    "        and pass kwargs as hyperparameters model\"\"\"\n",
    "        self.model_type=None\n",
    "        pass\n",
    "    \n",
    "    def fit(self, dataset: InterfaceData) -> None:\n",
    "        \"\"\"Apply model specific fit function\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, dataset: InterfaceData) -> Union[tuple[DataFrame, DataFrame], DataFrame]:\n",
    "        \"\"\"Return one or two pandas dataframes with predictions\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, dataset: InterfaceData, preds) -> None:\n",
    "        \"\"\"Calculate standard evaluation scores for model and save in outputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def score(self, dataset: InterfaceData) -> float:\n",
    "        \"\"\"Get singular score on relevant piece of data, depends of model type\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_class_names(self, dataset: InterfaceData) -> list:\n",
    "        \"\"\"Return original class names in list or [0,1] for binary datasets\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "To add a new model \n",
    "#### Step 1\n",
    "Extend the InterfaceModel class in a python file under `src/models/`. \n",
    "\n",
    "#### Step 2\n",
    "Implement the different functions and the constructor of your subclass. \n",
    "\n",
    "#### Step 3\n",
    "Add a new yaml file in the directory `configs/model`. \n",
    "It should target the new model class you created and add the hyperparameters of the model.\n",
    "\n",
    "The randomforest config looks like this:\n",
    "```yaml\n",
    "_target_: src.models.classification.RandomForest #name of class\n",
    "verbose: 0\n",
    "n_estimators: 10\n",
    "criterion: \"gini\"\n",
    "max_depth: null #null converted to None by Hydra\n",
    "max_features: \"sqrt\"\n",
    "max_leaf_nodes: null\n",
    "n_jobs: null\n",
    "class_weight: null\n",
    "```\n",
    "\n",
    "### Analysis functions\n",
    "Analysis functions follow the same interface\n",
    "```python\n",
    "def new_analysis_func(dataset: InterfaceData, model: InterfaceModel, **kwargs):\n",
    "    ...\n",
    "```\n",
    "Each analysis function should accept a dataset object, model object and any additional arguments that are needed.\n",
    "To add a new function\n",
    "\n",
    "#### Step 1\n",
    "Define a new function using the interface described above. `src.explain` contains a number of analysis functions.\n",
    "\n",
    "#### Step 2\n",
    "Add the analysis function to the config of a model under `configs/analysis/<model_name>.yaml`.\\\n",
    "It should target your newly made analysis function with *\\_target\\_*.\\\n",
    "It should also contain a data and model argument with *???*. This means it will get filled in by your chosen dataset and model objects in the experiment. If there are any additional arguments expected by the function you can add them under model.\n",
    "\n",
    "For example randomforest contains a function to explain feature importance with shap library.\\\n",
    "shaptotal can be any chosen name and will be displayed in the logs.\n",
    "```yaml\n",
    "shaptotal:\n",
    "    _target_: src.explain.shap_importance\n",
    "    dataset: ???\n",
    "    model: ???\n",
    "```\n",
    "\n",
    "\n",
    "## Relevant Documentation\n",
    "\n",
    "* Hydra\n",
    "    - nevergrad: [link](https://hydra.cc/docs/plugins/nevergrad_sweeper/)\n",
    "    - instantiate objects/call functions with hydra: [link](https://hydra.cc/docs/1.1/advanced/instantiate_objects/overview/)\n",
    "* Scikit-Learn [link](https://scikit-learn.org/stable/)\n",
    "* Python Outlier Detection (PyOD) [link](https://github.com/yzhao062/pyod)\n",
    "* Extreme Gradient Boosting (XGB) [link](https://github.com/dmlc/xgboost/)\n",
    "## References\n",
    "\n",
    "**Datasets**\n",
    "* nprobe UNSW'15 https://staff.itee.uq.edu.au/marius/NIDS_datasets/#RA4\n",
    "* tshark lemay https://github.com/antoine-lemay/Modbus_dataset\n",
    "* Proprietary Datasets generated for the GAICIA project, more information [link](https://gaicia.securityandprivacy.be/)\n",
    "\n",
    "**Papers**\n",
    "* [link](https://www.usenix.org/conference/cset16/workshop-program/presentation/lemay) Providing SCADA Network Data Sets for Intrusion Detection Research. (A. Lemay & J. M. Fernandez)\n",
    "* [link](https://arxiv.org/abs/1905.11757) Evaluation of machine learning based anomaly detection algorithms on modbus/TCP data set. (SD.  Anton & S. Kanoor)\n",
    "* [link](https://arxiv.org/abs/2011.09144) NetFlow Datasets for Machine Learning-based Network Intrusion Detection Systems. (M. Sarhan & S. Layeghy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad87776-1004-45bc-ac6d-b90962b4a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59227e7-554d-435e-bac9-82f9b86f1ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML-OT)",
   "language": "python",
   "name": "env_mlot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
